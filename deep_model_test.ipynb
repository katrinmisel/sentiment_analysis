{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tweet sentiment analysis with deep learning models\n",
        "\n",
        "In this notebook we will be testing several deep learning model configurations to find the best performing one on our set of 40.000 positive and 40.000 negative tweets.\n",
        "\n",
        "We will be attempting to guess positive (1) or negative (0) sentiment in a tweet."
      ],
      "metadata": {
        "id": "U4jqNkC2bNI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install necessary librairies\n",
        "\n",
        "!pip install emoji\n",
        "!pip install nltk\n",
        "!pip install tweet-preprocessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmoGPGoA6RjF",
        "outputId": "5170f041-1d83-4d75-ed4a-1e6c77a6a77a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.1.0.tar.gz (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 13.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.1.0-py3-none-any.whl size=212392 sha256=e011892c448ac71618de47efe5f0e945d1275c90f9ab8dfb6f2ebfab55e124ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/75/99/51c2a119f4cfd3af7b49cc57e4f737bed7e40b348a85d82804\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary librairies\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import emoji\n",
        "import preprocessor as p\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "from keras.layers import Flatten, LSTM, Embedding, Dense, Bidirectional\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import Sequential\n",
        "\n",
        "# download important nltk packages\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# save requirements\n",
        "\n",
        "pip freeze > requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REFSX2i-6S7A",
        "outputId": "20be8c27-cf4f-4505-b7fa-9bfe02815ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "english_words = set(nltk.corpus.words.words())\n",
        "lem = nltk.stem.WordNetLemmatizer()\n",
        "tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]+')\n",
        "\n",
        "def text_cleaner(text):\n",
        "\n",
        "  text = emoji.demojize(text, delimiters=(\"\", \"\")) # demojize the emojis in the docs\n",
        "\n",
        "  text = text.lower() # to lowercase\n",
        "    \n",
        "  text = tokenizer.tokenize(text) # tokenize with regular expressions\n",
        "\n",
        "  text = [w for w in text if w not in stop_words] # remove stopwords\n",
        "\n",
        "  text = [w for w in text if w in english_words] # keep only english words\n",
        "\n",
        "  text = [lem.lemmatize(w) for w in text] # lemmatize\n",
        "\n",
        "  text = [w for w in text if len(w) > 2] # keep only words longer than 2 characters\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "gGjgb1OE6ocX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_dictionary_w2vec = dict()\n",
        "w2vec_file = open('/content/drive/MyDrive/Colab Notebooks/enwiki_20180420_100d.txt', encoding='utf-8')\n",
        "\n",
        "for line in w2vec_file:\n",
        "    records = line.strip().split(' ')\n",
        "    word = records[0]\n",
        "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary_w2vec[word] = vector_dimensions\n",
        "w2vec_file.close()"
      ],
      "metadata": {
        "id": "w7QOWYXc9Ej-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_dictionary_glove = dict()\n",
        "glove_file = open('/content/drive/MyDrive/Colab Notebooks/glove.twitter.27B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary_glove[word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "metadata": {
        "id": "Vs-v3Jr69OnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/katrinmisel/sentiment_analysis/master/sample_df.csv\")"
      ],
      "metadata": {
        "id": "KrSFjIPsBcAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSyqVcoVwHPk"
      },
      "outputs": [],
      "source": [
        "def deep_model_test(data, text_prep_function, embedding, model_type, output_filename):\n",
        "\n",
        "  print(output_filename)\n",
        "  performances = []\n",
        "\n",
        "  # split in X and y (target)\n",
        "\n",
        "  if (text_prep_function == 'homemade'): # if the model uses our handmade function\n",
        "    X = np.array(data.tweet.apply(lambda x: text_cleaner(x)))\n",
        "  elif (text_prep_function == 'tweet_preprocessor'): # if the model uses the tweet preprocessor python library\n",
        "    X = np.array(data.tweet.apply(lambda x: p.clean(x)))\n",
        "  else:\n",
        "    X = np.array(data.tweet) #  no preprocessing at all\n",
        "\n",
        "  y = np.array(data.target)\n",
        "\n",
        "  # train test split with 30% test size\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "  # tokenize with keras\n",
        "\n",
        "  keras_tokenizer = Tokenizer(num_words=5000)\n",
        "  keras_tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "  # transform texts to sequences and pad sequences to the same length (100)\n",
        "\n",
        "  X_train = keras_tokenizer.texts_to_sequences(X_train)\n",
        "  X_test = keras_tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "  X_train = pad_sequences(X_train, padding='post', maxlen=100)\n",
        "  X_test = pad_sequences(X_test, padding='post', maxlen=100)\n",
        "\n",
        "  vocab_size = len(keras_tokenizer.word_index) + 1\n",
        "\n",
        "  if (embedding=='glove'): # create a GloVe embedding matrix\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, 100))\n",
        "    for word, index in keras_tokenizer.word_index.items():\n",
        "        embedding_vector = embeddings_dictionary_glove.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "  \n",
        "  else: # create a Wiki2Vec embedding matrix\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, 100))\n",
        "    for word, index in keras_tokenizer.word_index.items():\n",
        "        embedding_vector = embeddings_dictionary_w2vec.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "\n",
        "  # both our models are sequential keras models with a first embedding layer with the embedding matrix we created as weights\n",
        "\n",
        "  model = Sequential()\n",
        "  embedding_layer = Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=100, trainable=False)\n",
        "  model.add(embedding_layer)\n",
        "\n",
        "  # if model is simple, add a Flatten and a Dense layer\n",
        "\n",
        "  if (model_type == 'simple'):\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # else if model is advanced, add a bidirectional LSTM layer and a Dense layer\n",
        "\n",
        "  else:\n",
        "    # !!! since we are using dropout, our training accuracy will be lower than our validation accuracy\n",
        "    model.add(Bidirectional(LSTM(64, dropout=0.5, recurrent_dropout=0.5))) \n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # use callbacks: earlystopping and modelcheckpoint that saves our model at its best performance\n",
        "\n",
        "  es = EarlyStopping(monitor='val_accuracy', patience=3, verbose=0)\n",
        "  mc = ModelCheckpoint(output_filename, monitor='val_accuracy', patience=3, verbose=0)\n",
        "\n",
        "  # fit our model and time the training\n",
        "\n",
        "  start = time.time()\n",
        "  history = model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.3, callbacks=[es, mc])\n",
        "  end = time.time()\n",
        "\n",
        "  # evaluate our model on the validation set and record performances\n",
        "\n",
        "  score = model.evaluate(X_test, y_test, verbose=0)\n",
        "  loss = score[0]\n",
        "  accuracy = score[1]\n",
        "  training_time = end - start\n",
        "\n",
        "  performances.append(output_filename)\n",
        "  performances.append(loss)\n",
        "  performances.append(accuracy)\n",
        "  performances.append(training_time)\n",
        "\n",
        "  return performances # output a list with the name of the model, the validation loss, the validation accuracy and the training time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will check the performance of several models with our function.\n",
        "\n",
        "\n",
        "*   Simple neural network vs. bidirectional LSTM\n",
        "*   GloVe vs. Wiki2Vec encoding\n",
        "*   Tweet preprocessor library vs. our own text cleaning function"
      ],
      "metadata": {
        "id": "BkTInxHyZcfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perf_simple_homemade_wiki2vec = deep_model_test(data=df, text_prep_function='homemade', embedding='wiki2vec', model_type='simple', output_filename='perf_simple_homemade_wiki2vec.h5')\n",
        "perf_simple_homemade_glove = deep_model_test(data=df, text_prep_function='homemade', embedding='glove', model_type='simple', output_filename='perf_simple_homemade_glove.h5')\n",
        "perf_simple_tweetprep_wiki2vec = deep_model_test(data=df, text_prep_function='tweet_preprocessor', embedding='wiki2vec', model_type='simple', output_filename='perf_simple_tweetprep_wiki2vec.h5')\n",
        "perf_simple_tweetprep_glove = deep_model_test(data=df, text_prep_function='tweet_preprocessor', embedding='glove', model_type='simple', output_filename='perf_simple_tweetprep_glove.h5')\n",
        "\n",
        "perf_advanced_homemade_wiki2vec = deep_model_test(data=df, text_prep_function='homemade', embedding='wiki2vec', model_type='advanced', output_filename='perf_advanced_homemade_wiki2vec.h5')\n",
        "perf_advanced_homemade_glove = deep_model_test(data=df, text_prep_function='homemade', embedding='glove', model_type='advanced', output_filename='perf_advanced_homemade_glove.h5')\n",
        "perf_advanced_tweetprep_wiki2vec = deep_model_test(data=df, text_prep_function='tweet_preprocessor', embedding='wiki2vec', model_type='advanced', output_filename='perf_advanced_tweetprep_wiki2vec.h5')\n",
        "perf_advanced_tweetprep_glove = deep_model_test(data=df, text_prep_function='tweet_preprocessor', embedding='glove', model_type='advanced', output_filename='perf_advanced_tweetprep_glove.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRHPDlSbHEuF",
        "outputId": "818c10d0-50bd-46ca-db39-6b88684f080a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perf_simple_homemade_wiki2vec.h5\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 5ms/step - loss: 0.6707 - accuracy: 0.6070 - val_loss: 0.6570 - val_accuracy: 0.6248\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6412 - accuracy: 0.6514 - val_loss: 0.6417 - val_accuracy: 0.6414\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6263 - accuracy: 0.6624 - val_loss: 0.6339 - val_accuracy: 0.6461\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6172 - accuracy: 0.6684 - val_loss: 0.6298 - val_accuracy: 0.6504\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6108 - accuracy: 0.6730 - val_loss: 0.6260 - val_accuracy: 0.6537\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6061 - accuracy: 0.6750 - val_loss: 0.6243 - val_accuracy: 0.6550\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6026 - accuracy: 0.6768 - val_loss: 0.6229 - val_accuracy: 0.6565\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6000 - accuracy: 0.6769 - val_loss: 0.6225 - val_accuracy: 0.6580\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5974 - accuracy: 0.6802 - val_loss: 0.6227 - val_accuracy: 0.6564\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5953 - accuracy: 0.6819 - val_loss: 0.6217 - val_accuracy: 0.6569\n",
            "perf_simple_homemade_glove.h5\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 5ms/step - loss: 0.6295 - accuracy: 0.6550 - val_loss: 0.5982 - val_accuracy: 0.6792\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5843 - accuracy: 0.6951 - val_loss: 0.5848 - val_accuracy: 0.6900\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5712 - accuracy: 0.7046 - val_loss: 0.5777 - val_accuracy: 0.6923\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5636 - accuracy: 0.7072 - val_loss: 0.5751 - val_accuracy: 0.6931\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5590 - accuracy: 0.7108 - val_loss: 0.5742 - val_accuracy: 0.6957\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5553 - accuracy: 0.7128 - val_loss: 0.5737 - val_accuracy: 0.6968\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5528 - accuracy: 0.7142 - val_loss: 0.5734 - val_accuracy: 0.6954\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5506 - accuracy: 0.7163 - val_loss: 0.5734 - val_accuracy: 0.7008\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5489 - accuracy: 0.7170 - val_loss: 0.5731 - val_accuracy: 0.6971\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5475 - accuracy: 0.7164 - val_loss: 0.5747 - val_accuracy: 0.6940\n",
            "perf_simple_tweetprep_wiki2vec.h5\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 5ms/step - loss: 0.6669 - accuracy: 0.6016 - val_loss: 0.6495 - val_accuracy: 0.6448\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6674 - val_loss: 0.6310 - val_accuracy: 0.6615\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6082 - accuracy: 0.6865 - val_loss: 0.6244 - val_accuracy: 0.6594\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5941 - accuracy: 0.6964 - val_loss: 0.6161 - val_accuracy: 0.6724\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 1s 7ms/step - loss: 0.5846 - accuracy: 0.7009 - val_loss: 0.6129 - val_accuracy: 0.6788\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 1s 6ms/step - loss: 0.5759 - accuracy: 0.7088 - val_loss: 0.6104 - val_accuracy: 0.6796\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5687 - accuracy: 0.7151 - val_loss: 0.6109 - val_accuracy: 0.6758\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5639 - accuracy: 0.7167 - val_loss: 0.6077 - val_accuracy: 0.6788\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5589 - accuracy: 0.7189 - val_loss: 0.6078 - val_accuracy: 0.6796\n",
            "perf_simple_tweetprep_glove.h5\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 5ms/step - loss: 0.6206 - accuracy: 0.6626 - val_loss: 0.5822 - val_accuracy: 0.7087\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5606 - accuracy: 0.7233 - val_loss: 0.5657 - val_accuracy: 0.7180\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5400 - accuracy: 0.7379 - val_loss: 0.5589 - val_accuracy: 0.7190\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5275 - accuracy: 0.7457 - val_loss: 0.5573 - val_accuracy: 0.7195\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5187 - accuracy: 0.7503 - val_loss: 0.5558 - val_accuracy: 0.7280\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5117 - accuracy: 0.7541 - val_loss: 0.5554 - val_accuracy: 0.7271\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5067 - accuracy: 0.7580 - val_loss: 0.5561 - val_accuracy: 0.7224\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5029 - accuracy: 0.7576 - val_loss: 0.5576 - val_accuracy: 0.7242\n",
            "perf_advanced_homemade_wiki2vec.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "154/154 [==============================] - 113s 701ms/step - loss: 0.6640 - accuracy: 0.5905 - val_loss: 0.6365 - val_accuracy: 0.6483\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 110s 717ms/step - loss: 0.6397 - accuracy: 0.6329 - val_loss: 0.6200 - val_accuracy: 0.6588\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 107s 696ms/step - loss: 0.6313 - accuracy: 0.6417 - val_loss: 0.6104 - val_accuracy: 0.6675\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 111s 719ms/step - loss: 0.6256 - accuracy: 0.6480 - val_loss: 0.6089 - val_accuracy: 0.6687\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 109s 705ms/step - loss: 0.6164 - accuracy: 0.6555 - val_loss: 0.6002 - val_accuracy: 0.6726\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 111s 719ms/step - loss: 0.6125 - accuracy: 0.6604 - val_loss: 0.6007 - val_accuracy: 0.6700\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 106s 691ms/step - loss: 0.6093 - accuracy: 0.6598 - val_loss: 0.5889 - val_accuracy: 0.6813\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 110s 713ms/step - loss: 0.6033 - accuracy: 0.6676 - val_loss: 0.5871 - val_accuracy: 0.6775\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 105s 683ms/step - loss: 0.5985 - accuracy: 0.6699 - val_loss: 0.5885 - val_accuracy: 0.6811\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 109s 709ms/step - loss: 0.5928 - accuracy: 0.6765 - val_loss: 0.5776 - val_accuracy: 0.6890\n",
            "perf_advanced_homemade_glove.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "154/154 [==============================] - 112s 695ms/step - loss: 0.6208 - accuracy: 0.6533 - val_loss: 0.5708 - val_accuracy: 0.7015\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 111s 718ms/step - loss: 0.5869 - accuracy: 0.6846 - val_loss: 0.5598 - val_accuracy: 0.7086\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 108s 700ms/step - loss: 0.5813 - accuracy: 0.6895 - val_loss: 0.5540 - val_accuracy: 0.7137\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 111s 722ms/step - loss: 0.5743 - accuracy: 0.6933 - val_loss: 0.5510 - val_accuracy: 0.7142\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 107s 696ms/step - loss: 0.5692 - accuracy: 0.6964 - val_loss: 0.5457 - val_accuracy: 0.7174\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 110s 716ms/step - loss: 0.5618 - accuracy: 0.7021 - val_loss: 0.5433 - val_accuracy: 0.7195\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 106s 690ms/step - loss: 0.5590 - accuracy: 0.7063 - val_loss: 0.5407 - val_accuracy: 0.7164\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 109s 705ms/step - loss: 0.5538 - accuracy: 0.7080 - val_loss: 0.5389 - val_accuracy: 0.7207\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 110s 718ms/step - loss: 0.5548 - accuracy: 0.7110 - val_loss: 0.5380 - val_accuracy: 0.7202\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 108s 700ms/step - loss: 0.5501 - accuracy: 0.7117 - val_loss: 0.5354 - val_accuracy: 0.7218\n",
            "perf_advanced_tweetprep_wiki2vec.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "154/154 [==============================] - 111s 690ms/step - loss: 0.6569 - accuracy: 0.6130 - val_loss: 0.6253 - val_accuracy: 0.6656\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 109s 711ms/step - loss: 0.6271 - accuracy: 0.6494 - val_loss: 0.5988 - val_accuracy: 0.6900\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 106s 689ms/step - loss: 0.6188 - accuracy: 0.6527 - val_loss: 0.5938 - val_accuracy: 0.6896\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 108s 702ms/step - loss: 0.6137 - accuracy: 0.6576 - val_loss: 0.5995 - val_accuracy: 0.6742\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 106s 688ms/step - loss: 0.6027 - accuracy: 0.6689 - val_loss: 0.5830 - val_accuracy: 0.6935\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 107s 697ms/step - loss: 0.5957 - accuracy: 0.6754 - val_loss: 0.5726 - val_accuracy: 0.7110\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 107s 693ms/step - loss: 0.5902 - accuracy: 0.6813 - val_loss: 0.5696 - val_accuracy: 0.7146\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 107s 695ms/step - loss: 0.5805 - accuracy: 0.6917 - val_loss: 0.5566 - val_accuracy: 0.7148\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 109s 709ms/step - loss: 0.5753 - accuracy: 0.6958 - val_loss: 0.5491 - val_accuracy: 0.7245\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 106s 689ms/step - loss: 0.5720 - accuracy: 0.6957 - val_loss: 0.5483 - val_accuracy: 0.7265\n",
            "perf_advanced_tweetprep_glove.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "154/154 [==============================] - 111s 692ms/step - loss: 0.6078 - accuracy: 0.6680 - val_loss: 0.5397 - val_accuracy: 0.7331\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 108s 705ms/step - loss: 0.5657 - accuracy: 0.7057 - val_loss: 0.5266 - val_accuracy: 0.7412\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 105s 683ms/step - loss: 0.5500 - accuracy: 0.7178 - val_loss: 0.5180 - val_accuracy: 0.7404\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 108s 701ms/step - loss: 0.5433 - accuracy: 0.7229 - val_loss: 0.5142 - val_accuracy: 0.7530\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 106s 691ms/step - loss: 0.5363 - accuracy: 0.7271 - val_loss: 0.5018 - val_accuracy: 0.7537\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 107s 695ms/step - loss: 0.5297 - accuracy: 0.7292 - val_loss: 0.4966 - val_accuracy: 0.7630\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 107s 695ms/step - loss: 0.5226 - accuracy: 0.7365 - val_loss: 0.4930 - val_accuracy: 0.7661\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 112s 727ms/step - loss: 0.5184 - accuracy: 0.7422 - val_loss: 0.4909 - val_accuracy: 0.7635\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 130s 846ms/step - loss: 0.5141 - accuracy: 0.7437 - val_loss: 0.4898 - val_accuracy: 0.7619\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 130s 846ms/step - loss: 0.5073 - accuracy: 0.7443 - val_loss: 0.4831 - val_accuracy: 0.7651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we see that GloVe embedding works better than Wiki2Vec and the tweet preprocessor works better than the function we wrote, let's see if not cleaning the tweets at all could actually help our model.We test therefore with GloVe embedding on our simple and LSTM model."
      ],
      "metadata": {
        "id": "IuHAIvShZFNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perf_simple_noclean_glove = deep_model_test(data=df, text_prep_function='none', embedding='glove', model_type='simple', output_filename='perf_simple_noclean_glove.h5')\n",
        "perf_advanced_noclean_glove = deep_model_test(data=df, text_prep_function='none', embedding='glove', model_type='advanced', output_filename='perf_advanced_noclean_glove.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_sP1AoAYt-j",
        "outputId": "ca88588f-8613-4da0-909b-42f20fc3fad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perf_simple_noclean_glove.h5\n",
            "Epoch 1/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.6202 - accuracy: 0.6653 - val_loss: 0.5901 - val_accuracy: 0.6881\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5609 - accuracy: 0.7214 - val_loss: 0.5662 - val_accuracy: 0.7155\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5409 - accuracy: 0.7346 - val_loss: 0.5598 - val_accuracy: 0.7218\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5284 - accuracy: 0.7434 - val_loss: 0.5577 - val_accuracy: 0.7230\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5204 - accuracy: 0.7500 - val_loss: 0.5587 - val_accuracy: 0.7210\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5152 - accuracy: 0.7511 - val_loss: 0.5563 - val_accuracy: 0.7258\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5086 - accuracy: 0.7552 - val_loss: 0.5558 - val_accuracy: 0.7254\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5051 - accuracy: 0.7589 - val_loss: 0.5564 - val_accuracy: 0.7267\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.5017 - accuracy: 0.7604 - val_loss: 0.5574 - val_accuracy: 0.7271\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 1s 4ms/step - loss: 0.4983 - accuracy: 0.7631 - val_loss: 0.5685 - val_accuracy: 0.7171\n",
            "perf_advanced_noclean_glove.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "154/154 [==============================] - 153s 960ms/step - loss: 0.6101 - accuracy: 0.6650 - val_loss: 0.5421 - val_accuracy: 0.7279\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 138s 896ms/step - loss: 0.5645 - accuracy: 0.7089 - val_loss: 0.5240 - val_accuracy: 0.7381\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 129s 835ms/step - loss: 0.5515 - accuracy: 0.7154 - val_loss: 0.5145 - val_accuracy: 0.7485\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 109s 710ms/step - loss: 0.5438 - accuracy: 0.7208 - val_loss: 0.5181 - val_accuracy: 0.7374\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 109s 711ms/step - loss: 0.5379 - accuracy: 0.7252 - val_loss: 0.5067 - val_accuracy: 0.7530\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 106s 691ms/step - loss: 0.5283 - accuracy: 0.7306 - val_loss: 0.5047 - val_accuracy: 0.7498\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 136s 881ms/step - loss: 0.5239 - accuracy: 0.7330 - val_loss: 0.4944 - val_accuracy: 0.7573\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 127s 828ms/step - loss: 0.5161 - accuracy: 0.7401 - val_loss: 0.4903 - val_accuracy: 0.7617\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 111s 723ms/step - loss: 0.5130 - accuracy: 0.7401 - val_loss: 0.4872 - val_accuracy: 0.7675\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 112s 727ms/step - loss: 0.5101 - accuracy: 0.7417 - val_loss: 0.4874 - val_accuracy: 0.7639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_perfs = [perf_simple_homemade_wiki2vec,\n",
        "               perf_simple_homemade_glove,\n",
        "               perf_simple_tweetprep_wiki2vec,\n",
        "               perf_simple_tweetprep_glove,\n",
        "               perf_simple_noclean_glove,\n",
        "               perf_advanced_homemade_wiki2vec,\n",
        "               perf_advanced_homemade_glove,\n",
        "               perf_advanced_tweetprep_wiki2vec,\n",
        "               perf_advanced_tweetprep_glove,\n",
        "               perf_advanced_noclean_glove]"
      ],
      "metadata": {
        "id": "NZ73tc4fdvXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_comparison = pd.DataFrame(columns=['Name', 'Loss', 'Accuracy', 'Training time'], index=range(0,len(model_perfs)))\n",
        "\n",
        "for model in model_perfs:\n",
        "  i = model_perfs.index(model)\n",
        "  model_comparison['Name'][i] = model[0]\n",
        "  model_comparison['Loss'][i] = model[1]\n",
        "  model_comparison['Accuracy'][i] = model[2]\n",
        "  model_comparison['Training time'][i] = model[3]\n",
        "\n",
        "model_comparison"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "w3KpTDpEfASo",
        "outputId": "407dffbe-b932-47a2-e2a9-990aa4621348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  Name      Loss  Accuracy Training time\n",
              "0     perf_simple_homemade_wiki2vec.h5  0.624562  0.643833      10.64547\n",
              "1        perf_simple_homemade_glove.h5  0.579164   0.69425     10.884079\n",
              "2    perf_simple_tweetprep_wiki2vec.h5   0.60659  0.674917      6.712278\n",
              "3       perf_simple_tweetprep_glove.h5  0.557438  0.718833      5.305058\n",
              "4         perf_simple_noclean_glove.h5  0.568705   0.71325     10.646015\n",
              "5   perf_advanced_homemade_wiki2vec.h5  0.582028  0.682833   1107.097661\n",
              "6      perf_advanced_homemade_glove.h5  0.539268   0.71925   1106.423211\n",
              "7  perf_advanced_tweetprep_wiki2vec.h5  0.550751  0.721667   1106.755139\n",
              "8     perf_advanced_tweetprep_glove.h5  0.484914  0.761417   1124.828516\n",
              "9       perf_advanced_noclean_glove.h5  0.489464  0.760417   1287.484096"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6aaa686a-e9c1-4ab1-83da-dbd1877a0205\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Training time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>perf_simple_homemade_wiki2vec.h5</td>\n",
              "      <td>0.624562</td>\n",
              "      <td>0.643833</td>\n",
              "      <td>10.64547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>perf_simple_homemade_glove.h5</td>\n",
              "      <td>0.579164</td>\n",
              "      <td>0.69425</td>\n",
              "      <td>10.884079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>perf_simple_tweetprep_wiki2vec.h5</td>\n",
              "      <td>0.60659</td>\n",
              "      <td>0.674917</td>\n",
              "      <td>6.712278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>perf_simple_tweetprep_glove.h5</td>\n",
              "      <td>0.557438</td>\n",
              "      <td>0.718833</td>\n",
              "      <td>5.305058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>perf_simple_noclean_glove.h5</td>\n",
              "      <td>0.568705</td>\n",
              "      <td>0.71325</td>\n",
              "      <td>10.646015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>perf_advanced_homemade_wiki2vec.h5</td>\n",
              "      <td>0.582028</td>\n",
              "      <td>0.682833</td>\n",
              "      <td>1107.097661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>perf_advanced_homemade_glove.h5</td>\n",
              "      <td>0.539268</td>\n",
              "      <td>0.71925</td>\n",
              "      <td>1106.423211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>perf_advanced_tweetprep_wiki2vec.h5</td>\n",
              "      <td>0.550751</td>\n",
              "      <td>0.721667</td>\n",
              "      <td>1106.755139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>perf_advanced_tweetprep_glove.h5</td>\n",
              "      <td>0.484914</td>\n",
              "      <td>0.761417</td>\n",
              "      <td>1124.828516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>perf_advanced_noclean_glove.h5</td>\n",
              "      <td>0.489464</td>\n",
              "      <td>0.760417</td>\n",
              "      <td>1287.484096</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6aaa686a-e9c1-4ab1-83da-dbd1877a0205')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6aaa686a-e9c1-4ab1-83da-dbd1877a0205 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6aaa686a-e9c1-4ab1-83da-dbd1877a0205');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_comparison.to_csv('model_comparison.csv')"
      ],
      "metadata": {
        "id": "r-WdurT1jXok"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}